{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74aa6fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "from langchain.schema import Document\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI,GoogleGenerativeAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "import os\n",
    "\n",
    "embedding_function = GoogleGenerativeAIEmbeddings(\n",
    "    model=\"models/embedding-001\", \n",
    "    google_api_key=os.getenv(\"GEMINI_API_KEY\")\n",
    "    )\n",
    "\n",
    "docs = [\n",
    "    Document(\n",
    "        page_content=\"Peak Performance Gym was founded in 2015 by former Olympic athlete Marcus Chen. With over 15 years of experience in professional athletics, Marcus established the gym to provide personalized fitness solutions for people of all levels. The gym spans 10,000 square feet and features state-of-the-art equipment.\",\n",
    "        metadata={\"source\": \"about.txt\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Peak Performance Gym is open Monday through Friday from 5:00 AM to 11:00 PM. On weekends, our hours are 7:00 AM to 9:00 PM. We remain closed on major national holidays. Members with Premium access can enter using their key cards 24/7, including holidays.\",\n",
    "        metadata={\"source\": \"hours.txt\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Our membership plans include: Basic (₹1,500/month) with access to gym floor and basic equipment; Standard (₹2,500/month) adds group classes and locker facilities; Premium (₹4,000/month) includes 24/7 access, personal training sessions, and spa facilities. We offer student and senior citizen discounts of 15% on all plans. Corporate partnerships are available for companies with 10+ employees joining.\",\n",
    "        metadata={\"source\": \"membership.txt\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Group fitness classes at Peak Performance Gym include Yoga (beginner, intermediate, advanced), HIIT, Zumba, Spin Cycling, CrossFit, and Pilates. Beginner classes are held every Monday and Wednesday at 6:00 PM. Intermediate and advanced classes are scheduled throughout the week. The full schedule is available on our mobile app or at the reception desk.\",\n",
    "        metadata={\"source\": \"classes.txt\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Personal trainers at Peak Performance Gym are all certified professionals with minimum 5 years of experience. Each new member receives a complimentary fitness assessment and one free session with a trainer. Our head trainer, Neha Kapoor, specializes in rehabilitation fitness and sports-specific training. Personal training sessions can be booked individually (₹800/session) or in packages of 10 (₹7,000) or 20 (₹13,000).\",\n",
    "        metadata={\"source\": \"trainers.txt\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Peak Performance Gym's facilities include a cardio zone with 30+ machines, strength training area, functional fitness space, dedicated yoga studio, spin class room, swimming pool (25m), sauna and steam rooms, juice bar, and locker rooms with shower facilities. Our equipment is replaced or upgraded every 3 years to ensure members have access to the latest fitness technology.\",\n",
    "        metadata={\"source\": \"facilities.txt\"}\n",
    "    )\n",
    "]\n",
    "\n",
    "db = Chroma.from_documents(docs, embedding_function)\n",
    "retriever = db.as_retriever(search_type=\"mmr\", search_kwargs = {\"k\": 4})\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    api_key=os.getenv(\"GEMINI_API_KEY\")\n",
    "    )\n",
    "\n",
    "template = \"\"\"Answer the question based on the following context and the Chathistory. Especially take the latest question into consideration:\n",
    "\n",
    "Chathistory: {history}\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "rag_chain = prompt | llm\n",
    "\n",
    "from typing import TypedDict, List\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage\n",
    "from langchain.schema import Document\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langgraph.graph import StateGraph, END\n",
    "import folium\n",
    "import io\n",
    "import base64\n",
    "import json\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: List[BaseMessage]\n",
    "    documents: List[Document]\n",
    "    on_topic: str\n",
    "    rephrased_question: str\n",
    "    proceed_to_generate: bool\n",
    "    rephrase_count: int\n",
    "    question: HumanMessage\n",
    "    # New fields for GIS\n",
    "    needs_gis: bool\n",
    "    gis_data: dict\n",
    "    map_html: str\n",
    "\n",
    "class GradeQuestion(BaseModel):\n",
    "    score: str = Field(\n",
    "        description=\"Question is about the specified topics? If yes -> 'Yes' if not -> 'No'\"\n",
    "    )\n",
    "\n",
    "class GISRequest(BaseModel):\n",
    "    needs_gis: str = Field(\n",
    "        description=\"Does the answer require a GIS map visualization? 'Yes' or 'No'\"\n",
    "    )\n",
    "    location_data: str = Field(\n",
    "        description=\"Extract location information if available (state, district, coordinates, etc.)\"\n",
    "    )\n",
    "\n",
    "def question_rewriter(state: AgentState):\n",
    "    print(f\"Entering question_rewriter with following state: {state}\")\n",
    "\n",
    "    # Reset state variables except for 'question' and 'messages'\n",
    "    state[\"documents\"] = []\n",
    "    state[\"on_topic\"] = \"\"\n",
    "    state[\"rephrased_question\"] = \"\"\n",
    "    state[\"proceed_to_generate\"] = False\n",
    "    state[\"rephrase_count\"] = 0\n",
    "    state[\"needs_gis\"] = False\n",
    "    state[\"gis_data\"] = {}\n",
    "    state[\"map_html\"] = \"\"\n",
    "\n",
    "    if \"messages\" not in state or state[\"messages\"] is None:\n",
    "        state[\"messages\"] = []\n",
    "\n",
    "    if state[\"question\"] not in state[\"messages\"]:\n",
    "        state[\"messages\"].append(state[\"question\"])\n",
    "\n",
    "    if len(state[\"messages\"]) > 1:\n",
    "        conversation = state[\"messages\"][:-1]\n",
    "        current_question = state[\"question\"].content\n",
    "        messages = [\n",
    "            SystemMessage(\n",
    "                content=\"You are a helpful assistant that rephrases the user's question to be a standalone question optimized for retrieval.\"\n",
    "            )\n",
    "        ]\n",
    "        messages.extend(conversation)\n",
    "        messages.append(HumanMessage(content=current_question))\n",
    "        rephrase_prompt = ChatPromptTemplate.from_messages(messages)\n",
    "        llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "        prompt = rephrase_prompt.format()\n",
    "        response = llm.invoke(prompt)\n",
    "        better_question = response.content.strip()\n",
    "        print(f\"question_rewriter: Rephrased question: {better_question}\")\n",
    "        state[\"rephrased_question\"] = better_question\n",
    "    else:\n",
    "        state[\"rephrased_question\"] = state[\"question\"].content\n",
    "    return state\n",
    "\n",
    "def question_classifier(state: AgentState):\n",
    "    print(\"Entering question_classifier\")\n",
    "    system_message = SystemMessage(\n",
    "        content=\"\"\" You are a classifier that determines whether a user's question is about one of the following topics \n",
    "        \n",
    "        1. Gym History & Founder\n",
    "        2. Operating Hours\n",
    "        3. Membership Plans \n",
    "        4. Fitness Classes\n",
    "        5. Personal Trainers\n",
    "        6. Facilities & Equipment\n",
    "        7. Anything else about Peak Performance Gym\n",
    "        \n",
    "        If the question IS about any of these topics, respond with 'Yes'. Otherwise, respond with 'No'.\n",
    "\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    human_message = HumanMessage(\n",
    "        content=f\"User question: {state['rephrased_question']}\"\n",
    "    )\n",
    "    grade_prompt = ChatPromptTemplate.from_messages([system_message, human_message])\n",
    "    llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "    structured_llm = llm.with_structured_output(GradeQuestion)\n",
    "    grader_llm = grade_prompt | structured_llm\n",
    "    result = grader_llm.invoke({})\n",
    "    state[\"on_topic\"] = result.score.strip()\n",
    "    print(f\"question_classifier: on_topic = {state['on_topic']}\")\n",
    "    return state\n",
    "\n",
    "def on_topic_router(state: AgentState):\n",
    "    print(\"Entering on_topic_router\")\n",
    "    on_topic = state.get(\"on_topic\", \"\").strip().lower()\n",
    "    if on_topic == \"yes\":\n",
    "        print(\"Routing to retrieve\")\n",
    "        return \"retrieve\"\n",
    "    else:\n",
    "        print(\"Routing to off_topic_response\")\n",
    "        return \"off_topic_response\"\n",
    "\n",
    "def retrieve(state: AgentState):\n",
    "    print(\"Entering retrieve\")\n",
    "    documents = retriever.invoke(state[\"rephrased_question\"])\n",
    "    print(f\"retrieve: Retrieved {len(documents)} documents\")\n",
    "    state[\"documents\"] = documents\n",
    "    return state\n",
    "\n",
    "class GradeDocument(BaseModel):\n",
    "    score: str = Field(\n",
    "        description=\"Document is relevant to the question? If yes -> 'Yes' if not -> 'No'\"\n",
    "    )\n",
    "\n",
    "def retrieval_grader(state: AgentState):\n",
    "    print(\"Entering retrieval_grader\")\n",
    "    system_message = SystemMessage(\n",
    "        content=\"\"\"You are a grader assessing the relevance of a retrieved document to a user question.\n",
    "Only answer with 'Yes' or 'No'.\n",
    "\n",
    "If the document contains information relevant to the user's question, respond with 'Yes'.\n",
    "Otherwise, respond with 'No'.\"\"\"\n",
    "    )\n",
    "\n",
    "    llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "    structured_llm = llm.with_structured_output(GradeDocument)\n",
    "\n",
    "    relevant_docs = []\n",
    "    for doc in state[\"documents\"]:\n",
    "        human_message = HumanMessage(\n",
    "            content=f\"User question: {state['rephrased_question']}\\n\\nRetrieved document:\\n{doc.page_content}\"\n",
    "        )\n",
    "        grade_prompt = ChatPromptTemplate.from_messages([system_message, human_message])\n",
    "        grader_llm = grade_prompt | structured_llm\n",
    "        result = grader_llm.invoke({})\n",
    "        print(\n",
    "            f\"Grading document: {doc.page_content[:30]}... Result: {result.score.strip()}\"\n",
    "        )\n",
    "        if result.score.strip().lower() == \"yes\":\n",
    "            relevant_docs.append(doc)\n",
    "    state[\"documents\"] = relevant_docs\n",
    "    state[\"proceed_to_generate\"] = len(relevant_docs) > 0\n",
    "    print(f\"retrieval_grader: proceed_to_generate = {state['proceed_to_generate']}\")\n",
    "    return state\n",
    "\n",
    "def proceed_router(state: AgentState):\n",
    "    print(\"Entering proceed_router\")\n",
    "    rephrase_count = state.get(\"rephrase_count\", 0)\n",
    "    if state.get(\"proceed_to_generate\", False):\n",
    "        print(\"Routing to generate_answer\")\n",
    "        return \"generate_answer\"\n",
    "    elif rephrase_count >= 2:\n",
    "        print(\"Maximum rephrase attempts reached. Cannot find relevant documents.\")\n",
    "        return \"cannot_answer\"\n",
    "    else:\n",
    "        print(\"Routing to refine_question\")\n",
    "        return \"refine_question\"\n",
    " \n",
    "def refine_question(state: AgentState):\n",
    "    print(\"Entering refine_question\")\n",
    "    rephrase_count = state.get(\"rephrase_count\", 0)\n",
    "    if rephrase_count >= 2:\n",
    "        print(\"Maximum rephrase attempts reached\")\n",
    "        return state\n",
    "    question_to_refine = state[\"rephrased_question\"]\n",
    "    system_message = SystemMessage(\n",
    "        content=\"\"\"You are a helpful assistant that slightly refines the user's question to improve retrieval results.\n",
    "Provide a slightly adjusted version of the question.\"\"\"\n",
    "    )\n",
    "    human_message = HumanMessage(\n",
    "        content=f\"Original question: {question_to_refine}\\n\\nProvide a slightly refined question.\"\n",
    "    )\n",
    "    refine_prompt = ChatPromptTemplate.from_messages([system_message, human_message])\n",
    "    llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "    prompt = refine_prompt.format()\n",
    "    response = llm.invoke(prompt)\n",
    "    refined_question = response.content.strip()\n",
    "    print(f\"refine_question: Refined question: {refined_question}\")\n",
    "    state[\"rephrased_question\"] = refined_question\n",
    "    state[\"rephrase_count\"] = rephrase_count + 1\n",
    "    return state\n",
    "\n",
    "def generate_answer(state: AgentState):\n",
    "    print(\"Entering generate_answer\")\n",
    "    if \"messages\" not in state or state[\"messages\"] is None:\n",
    "        raise ValueError(\"State must include 'messages' before generating an answer.\")\n",
    "\n",
    "    history = state[\"messages\"]\n",
    "    documents = state[\"documents\"]\n",
    "    rephrased_question = state[\"rephrased_question\"]\n",
    "\n",
    "    response = rag_chain.invoke(\n",
    "        {\"history\": history, \"context\": documents, \"question\": rephrased_question}\n",
    "    )\n",
    "\n",
    "    generation = response.content.strip()\n",
    "\n",
    "    state[\"messages\"].append(AIMessage(content=generation))\n",
    "    print(f\"generate_answer: Generated response: {generation}\")\n",
    "    return state\n",
    "\n",
    "def gis_classifier(state: AgentState):\n",
    "    \"\"\"Classify if the generated answer needs GIS visualization\"\"\"\n",
    "    print(\"Entering gis_classifier\")\n",
    "    \n",
    "    if not state[\"messages\"] or not isinstance(state[\"messages\"][-1], AIMessage):\n",
    "        state[\"needs_gis\"] = False\n",
    "        return state\n",
    "    \n",
    "    answer_content = state[\"messages\"][-1].content\n",
    "    question = state[\"rephrased_question\"]\n",
    "    \n",
    "    # GIS Classification prompt\n",
    "    system_message = SystemMessage(\n",
    "        content=\"\"\"You are a GIS classifier that determines if a groundwater-related answer would benefit from a map visualization.\n",
    "\n",
    "Look for:\n",
    "- Location-specific data (states, districts, coordinates)\n",
    "- Groundwater levels, extraction rates, or quality data with geographic context\n",
    "- Spatial comparisons between regions\n",
    "- Well locations or monitoring stations\n",
    "- Geographic distribution of water resources\n",
    "\n",
    "If the answer contains geographic/spatial information that would be enhanced by a map, respond with 'Yes'.\n",
    "Otherwise, respond with 'No'.\n",
    "\n",
    "Also extract any location information mentioned in the question or answer.\"\"\"\n",
    "    )\n",
    "    \n",
    "    human_message = HumanMessage(\n",
    "        content=f\"Question: {question}\\n\\nAnswer: {answer_content}\\n\\nDoes this need a GIS map visualization?\"\n",
    "    )\n",
    "    \n",
    "    gis_prompt = ChatPromptTemplate.from_messages([system_message, human_message])\n",
    "    llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "    structured_llm = llm.with_structured_output(GISRequest)\n",
    "    grader_llm = gis_prompt | structured_llm\n",
    "    result = grader_llm.invoke({})\n",
    "    \n",
    "    state[\"needs_gis\"] = result.needs_gis.strip().lower() == \"yes\"\n",
    "    if state[\"needs_gis\"]:\n",
    "        state[\"gis_data\"] = {\"location_info\": result.location_data.strip()}\n",
    "    \n",
    "    print(f\"gis_classifier: needs_gis = {state['needs_gis']}\")\n",
    "    return state\n",
    "\n",
    "def generate_gis_map(state: AgentState):\n",
    "    \"\"\"Generate GIS map for groundwater data\"\"\"\n",
    "    print(\"Entering generate_gis_map\")\n",
    "    \n",
    "    if not state.get(\"needs_gis\", False):\n",
    "        return state\n",
    "    \n",
    "    # Extract location info\n",
    "    location_info = state.get(\"gis_data\", {}).get(\"location_info\", \"\")\n",
    "    \n",
    "    # Create sample GIS data based on location (in real implementation, query your groundwater database)\n",
    "    sample_points = [\n",
    "        {\"name\": \"Karnataka\", \"lat\": 15.3173, \"lon\": 75.7139, \"value\": 218.54, \"category\": \"Safe\"},\n",
    "        {\"name\": \"Maharashtra\", \"lat\": 19.7515, \"lon\": 75.7139, \"value\": 256.66, \"category\": \"Semi-Critical\"},\n",
    "        {\"name\": \"Gujarat\", \"lat\": 23.0225, \"lon\": 72.5714, \"value\": 145.55, \"category\": \"Critical\"},\n",
    "        {\"name\": \"Rajasthan\", \"lat\": 27.0238, \"lon\": 74.2179, \"value\": 286.15, \"category\": \"Over-Exploited\"},\n",
    "    ]\n",
    "    \n",
    "    # Create Folium map centered on India\n",
    "    m = folium.Map(location=[22.0, 79.0], zoom_start=5, tiles='OpenStreetMap')\n",
    "    \n",
    "    # Define colors for different categories\n",
    "    color_map = {\n",
    "        \"Safe\": \"green\",\n",
    "        \"Semi-Critical\": \"orange\", \n",
    "        \"Critical\": \"red\",\n",
    "        \"Over-Exploited\": \"darkred\"\n",
    "    }\n",
    "    \n",
    "    # Add points to map\n",
    "    for point in sample_points:\n",
    "        color = color_map.get(point[\"category\"], \"blue\")\n",
    "        folium.CircleMarker(\n",
    "            location=[point[\"lat\"], point[\"lon\"]],\n",
    "            radius=8 + point[\"value\"] * 0.02,\n",
    "            color=color,\n",
    "            fill=True,\n",
    "            fillColor=color,\n",
    "            fillOpacity=0.7,\n",
    "            popup=f\"<b>{point['name']}</b><br>\"\n",
    "                  f\"Recharge: {point['value']} MCM<br>\"\n",
    "                  f\"Category: {point['category']}\",\n",
    "            tooltip=point[\"name\"]\n",
    "        ).add_to(m)\n",
    "    \n",
    "    # Add legend\n",
    "    legend_html = \"\"\"\n",
    "    <div style=\"position: fixed; \n",
    "                bottom: 50px; left: 50px; width: 150px; height: 90px; \n",
    "                background-color: white; border:2px solid grey; z-index:9999; \n",
    "                font-size:14px; padding: 10px\">\n",
    "    <p><b>Groundwater Status</b></p>\n",
    "    <p><i class=\"fa fa-circle\" style=\"color:green\"></i> Safe</p>\n",
    "    <p><i class=\"fa fa-circle\" style=\"color:orange\"></i> Semi-Critical</p>\n",
    "    <p><i class=\"fa fa-circle\" style=\"color:red\"></i> Critical</p>\n",
    "    <p><i class=\"fa fa-circle\" style=\"color:darkred\"></i> Over-Exploited</p>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    m.get_root().html.add_child(folium.Element(legend_html))\n",
    "    \n",
    "    # Convert to HTML string\n",
    "    html_str = m._repr_html_()\n",
    "    state[\"map_html\"] = html_str\n",
    "    \n",
    "    # Update the last AI message to include map reference\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    updated_content = f\"{last_message.content}\\n\\n[Interactive GIS Map Generated - showing groundwater status across regions]\"\n",
    "    state[\"messages\"][-1] = AIMessage(content=updated_content)\n",
    "    \n",
    "    print(\"generate_gis_map: GIS map generated successfully\")\n",
    "    return state\n",
    "\n",
    "def gis_router(state: AgentState):\n",
    "    \"\"\"Route based on GIS classification\"\"\"\n",
    "    print(\"Entering gis_router\")\n",
    "    if state.get(\"needs_gis\", False):\n",
    "        print(\"Routing to generate_gis_map\")\n",
    "        return \"generate_gis_map\"\n",
    "    else:\n",
    "        print(\"Routing to END\")\n",
    "        return END\n",
    "\n",
    "def cannot_answer(state: AgentState):\n",
    "    print(\"Entering cannot_answer\")\n",
    "    if \"messages\" not in state or state[\"messages\"] is None:\n",
    "        state[\"messages\"] = []\n",
    "    state[\"messages\"].append(\n",
    "        AIMessage(\n",
    "            content=\"I'm sorry, but I cannot find the information you're looking for.\"\n",
    "        )\n",
    "    )\n",
    "    return state\n",
    "\n",
    "def off_topic_response(state: AgentState):\n",
    "    print(\"Entering off_topic_response\")\n",
    "    if \"messages\" not in state or state[\"messages\"] is None:\n",
    "        state[\"messages\"] = []\n",
    "    state[\"messages\"].append(AIMessage(content=\"I'm sorry! I cannot answer this question!\"))\n",
    "    return state\n",
    "\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "checkpointer = MemorySaver()\n",
    "\n",
    "# Workflow\n",
    "workflow = StateGraph(AgentState)\n",
    "workflow.add_node(\"question_rewriter\", question_rewriter)\n",
    "workflow.add_node(\"question_classifier\", question_classifier)\n",
    "workflow.add_node(\"off_topic_response\", off_topic_response)\n",
    "workflow.add_node(\"retrieve\", retrieve)\n",
    "workflow.add_node(\"retrieval_grader\", retrieval_grader)\n",
    "workflow.add_node(\"generate_answer\", generate_answer)\n",
    "workflow.add_node(\"refine_question\", refine_question)\n",
    "workflow.add_node(\"cannot_answer\", cannot_answer)\n",
    "\n",
    "# New GIS nodes\n",
    "workflow.add_node(\"gis_classifier\", gis_classifier)\n",
    "workflow.add_node(\"generate_gis_map\", generate_gis_map)\n",
    "\n",
    "workflow.add_edge(\"question_rewriter\", \"question_classifier\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"question_classifier\",\n",
    "    on_topic_router,\n",
    "    {\n",
    "        \"retrieve\": \"retrieve\",\n",
    "        \"off_topic_response\": \"off_topic_response\",\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"retrieve\", \"retrieval_grader\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"retrieval_grader\",\n",
    "    proceed_router,\n",
    "    {\n",
    "        \"generate_answer\": \"generate_answer\",\n",
    "        \"refine_question\": \"refine_question\",\n",
    "        \"cannot_answer\": \"cannot_answer\",\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"refine_question\", \"retrieve\")\n",
    "\n",
    "# Modified: After generate_answer, go to GIS classifier\n",
    "workflow.add_edge(\"generate_answer\", \"gis_classifier\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"gis_classifier\",\n",
    "    gis_router,\n",
    "    {\n",
    "        \"generate_gis_map\": \"generate_gis_map\",\n",
    "        END: END,\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"generate_gis_map\", END)\n",
    "\n",
    "workflow.add_edge(\"cannot_answer\", END)\n",
    "workflow.add_edge(\"off_topic_response\", END)\n",
    "workflow.set_entry_point(\"question_rewriter\")\n",
    "\n",
    "graph = workflow.compile(checkpointer=checkpointer)\n",
    "\n",
    "# Test the workflow\n",
    "if __name__ == \"__main__\":\n",
    "    from IPython.display import Image, display\n",
    "    from langchain_core.runnables.graph import MermaidDrawMethod\n",
    "\n",
    "    display(\n",
    "        Image(\n",
    "            graph.get_graph().draw_mermaid_png(\n",
    "                draw_method=MermaidDrawMethod.API,\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Test with a spatial query\n",
    "    input_data = {\n",
    "        \"question\": HumanMessage(content=\"Show me groundwater levels in Karnataka and Maharashtra\")\n",
    "    }\n",
    "    result = graph.invoke(input=input_data, config={\"configurable\": {\"thread_id\": 1}})\n",
    "    \n",
    "    if result.get(\"map_html\"):\n",
    "        print(\"GIS Map generated successfully!\")\n",
    "        # In a real application, you would serve this HTML to the frontend\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0991766c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\prita\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain_pinecone\\__init__.py:3: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  from langchain_pinecone.vectorstores import Pinecone, PineconeVectorStore\n",
      "INFO:__main__:Initializing Pinecone...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "init is no longer a top-level attribute of the pinecone package.\n\nPlease create an instance of the Pinecone class instead.\n\nExample:\n\n    import os\n    from pinecone import Pinecone, ServerlessSpec\n\n    pc = Pinecone(\n        api_key=os.environ.get(\"PINECONE_API_KEY\")\n    )\n\n    # Now do stuff\n    if 'my_index' not in pc.list_indexes().names():\n        pc.create_index(\n            name='my_index',\n            dimension=1536,\n            metric='euclidean',\n            spec=ServerlessSpec(\n                cloud='aws',\n                region='us-west-2'\n            )\n        )\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 79\u001b[39m\n\u001b[32m     76\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m'\u001b[39m\u001b[33mPlease set PINECONE_API_KEY in your environment or .env file\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     78\u001b[39m logger.info(\u001b[33m'\u001b[39m\u001b[33mInitializing Pinecone...\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m \u001b[43mpinecone\u001b[49m\u001b[43m.\u001b[49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m=\u001b[49m\u001b[43mPINECONE_API_KEY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menvironment\u001b[49m\u001b[43m=\u001b[49m\u001b[43mPINECONE_ENV\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     81\u001b[39m \u001b[38;5;66;03m# get embedding dimension from embedding_function\u001b[39;00m\n\u001b[32m     82\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pinecone\\deprecation_warnings.py:40\u001b[39m, in \u001b[36minit\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     13\u001b[39m     example = \u001b[33m\"\"\"\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[33m    import os\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[33m    from pinecone import Pinecone, ServerlessSpec\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m     31\u001b[39m \u001b[33m        )\u001b[39m\n\u001b[32m     32\u001b[39m \u001b[33m\"\"\"\u001b[39m\n\u001b[32m     33\u001b[39m     msg = \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\u001b[33minit is no longer a top-level attribute of the pinecone package.\u001b[39m\n\u001b[32m     34\u001b[39m \n\u001b[32m     35\u001b[39m \u001b[33mPlease create an instance of the Pinecone class instead.\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m     38\u001b[39m \u001b[38;5;132;01m{\u001b[39;00mexample\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[32m     39\u001b[39m \u001b[33m\"\"\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(msg)\n",
      "\u001b[31mAttributeError\u001b[39m: init is no longer a top-level attribute of the pinecone package.\n\nPlease create an instance of the Pinecone class instead.\n\nExample:\n\n    import os\n    from pinecone import Pinecone, ServerlessSpec\n\n    pc = Pinecone(\n        api_key=os.environ.get(\"PINECONE_API_KEY\")\n    )\n\n    # Now do stuff\n    if 'my_index' not in pc.list_indexes().names():\n        pc.create_index(\n            name='my_index',\n            dimension=1536,\n            metric='euclidean',\n            spec=ServerlessSpec(\n                cloud='aws',\n                region='us-west-2'\n            )\n        )\n\n"
     ]
    }
   ],
   "source": [
    "# INGRES_RAG_GIS_notebook.py\n",
    "# Full runnable notebook-style Python script for the INGRES RAG + GIS workflow\n",
    "# - Uses Google Gemini (langchain_google_genai)\n",
    "# - Uses Pinecone vector store via langchain_pinecone wrapper\n",
    "# - Uses langgraph StateGraph to implement the workflow you designed\n",
    "#\n",
    "# NOTE: Before running, set environment variables in a .env file or your environment:\n",
    "#  - GEMINI_API_KEY\n",
    "#  - PINECONE_API_KEY\n",
    "#  - PINECONE_ENV (optional)\n",
    "#  - PINECONE_INDEX (optional, defaults to 'ingres-index')\n",
    "#\n",
    "# Install required packages first (run in the environment or a notebook cell):\n",
    "# !pip install -U pinecone langchain-pinecone langchain langchain-openai langchain-google-genai python-dotenv pandas folium langgraph\n",
    "\n",
    "# ---------------------------\n",
    "# Cell 1: Imports & env\n",
    "# ---------------------------\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import logging\n",
    "from typing import TypedDict, List\n",
    "\n",
    "import pandas as pd\n",
    "import folium\n",
    "\n",
    "# LangChain / LLM / Embedding\n",
    "from langchain.schema import Document\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "# from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Pinecone / LangChain-Pinecone\n",
    "import pinecone\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "# LangGraph\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# ---------------------------\n",
    "# Logging\n",
    "# ---------------------------\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# ---------------------------\n",
    "# Cell 2: LLMs and embeddings\n",
    "# ---------------------------\n",
    "GEMINI_API_KEY = os.getenv('GEMINI_API_KEY')\n",
    "if not GEMINI_API_KEY:\n",
    "    raise RuntimeError('Please set GEMINI_API_KEY in your environment or .env file')\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", api_key=GEMINI_API_KEY)\n",
    "embedding_function = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\", google_api_key=GEMINI_API_KEY)\n",
    "\n",
    "# Also prepare a lightweight ChatOpenAI wrapper for short classification tasks (you may adapt to your setup)\n",
    "# NOTE: depending on your local langchain_openai wrapper you might need to pass api_key or other args.\n",
    "chat_for_classify = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    api_key=os.getenv(\"GEMINI_API_KEY\")\n",
    "    )  \n",
    "\n",
    "# ---------------------------\n",
    "# Cell 3: Pinecone init and vectorstore\n",
    "# ---------------------------\n",
    "PINECONE_API_KEY = os.getenv('PINECONE_API_KEY')\n",
    "PINECONE_ENV = os.getenv('PINECONE_ENV')\n",
    "INDEX_NAME = os.getenv('PINECONE_INDEX', 'ingres-index')\n",
    "\n",
    "if not PINECONE_API_KEY:\n",
    "    raise RuntimeError('Please set PINECONE_API_KEY in your environment or .env file')\n",
    "\n",
    "logger.info('Initializing Pinecone...')\n",
    "pinecone.init(api_key=PINECONE_API_KEY, environment=PINECONE_ENV)\n",
    "\n",
    "# get embedding dimension from embedding_function\n",
    "try:\n",
    "    example_vec = embedding_function.embed_query('test')\n",
    "    DIM = len(example_vec)\n",
    "except Exception as ex:\n",
    "    logger.warning('Could not get embedding dim from embedding API (%s). Falling back to 1536', ex)\n",
    "    DIM = 1536\n",
    "\n",
    "if INDEX_NAME not in pinecone.list_indexes():\n",
    "    logger.info('Creating Pinecone index %s with dim=%s', INDEX_NAME, DIM)\n",
    "    pinecone.create_index(name=INDEX_NAME, dimension=DIM)\n",
    "else:\n",
    "    logger.info('Using existing Pinecone index %s', INDEX_NAME)\n",
    "\n",
    "# Create langchain_pinecone wrapper\n",
    "vectorstore = PineconeVectorStore(index_name=INDEX_NAME, embedding=embedding_function, pinecone_api_key=PINECONE_API_KEY)\n",
    "retriever = vectorstore.as_retriever(search_type='mmr', search_kwargs={'k': 4})\n",
    "\n",
    "# ---------------------------\n",
    "# Cell 4: CSV loader and upsert helper\n",
    "# ---------------------------\n",
    "\n",
    "def load_csv_as_docs(path: str, source_name: str) -> List[Document]:\n",
    "    \"\"\"Load a CSV and convert rows into Document objects.\"\"\"\n",
    "    df = pd.read_csv(path)\n",
    "    docs: List[Document] = []\n",
    "    for idx, row in df.iterrows():\n",
    "        content = '\\n'.join(f\"{col}: {row[col]}\" for col in df.columns)\n",
    "        docs.append(Document(page_content=content, metadata={\"source\": source_name, \"row\": int(idx)}))\n",
    "    return docs\n",
    "\n",
    "# Example: if you have the CSV files locally (comment/uncomment as needed)\n",
    "csv_paths = {\n",
    "    \"assessment_units\": \"ingres_assessment_units_2023.csv\",\n",
    "    \"trends\": \"groundwater_trends_2015_2023.csv\",\n",
    "    \"quality\": \"groundwater_quality_data.csv\",\n",
    "    \"realtime_dwlr\": \"dwlr_realtime_data.csv\",\n",
    "}\n",
    "\n",
    "all_docs: List[Document] = []\n",
    "for source, path in csv_paths.items():\n",
    "    if os.path.exists(path):\n",
    "        logger.info('Loading %s', path)\n",
    "        all_docs += load_csv_as_docs(path, source)\n",
    "    else:\n",
    "        logger.warning('CSV missing, skipping: %s', path)\n",
    "\n",
    "# Upsert into Pinecone (only if docs present)\n",
    "if all_docs:\n",
    "    texts = [d.page_content for d in all_docs]\n",
    "    metadatas = [d.metadata for d in all_docs]\n",
    "    vectorstore.add_texts(texts=texts, metadatas=metadatas)\n",
    "    logger.info('Upserted %d documents to vectorstore', len(texts))\n",
    "else:\n",
    "    logger.info('No local CSVs found; you can still add documents programmatically later.')\n",
    "\n",
    "# ---------------------------\n",
    "# Cell 5: RAG prompt and simple chain\n",
    "# ---------------------------\n",
    "template = \"\"\"Use INGRES data to answer:\\n\\nChat History:\\n{history}\\n\\nRetrieved Context:\\n{context}\\n\\nUser Question:\\n{question}\\n\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "rag_chain = prompt | llm\n",
    "\n",
    "# ---------------------------\n",
    "# Cell 6: Workflow: types, classifiers, nodes\n",
    "# ---------------------------\n",
    "class AgentState(TypedDict):\n",
    "    messages: List[BaseMessage]\n",
    "    documents: List[Document]\n",
    "    on_topic: str\n",
    "    rephrased_question: str\n",
    "    proceed_to_generate: bool\n",
    "    rephrase_count: int\n",
    "    question: HumanMessage\n",
    "    needs_gis: bool\n",
    "    gis_data: dict\n",
    "    map_html: str\n",
    "\n",
    "class GradeQuestion(BaseModel):\n",
    "    score: str = Field(description='Is user asking INGRES data? Yes/No')\n",
    "\n",
    "class GISRequest(BaseModel):\n",
    "    needs_gis: str = Field(description='Need GIS map? Yes/No')\n",
    "    location_data: str = Field(description='Extracted location info')\n",
    "\n",
    "# Node implementations follow (largely adapted from your original file)\n",
    "\n",
    "def question_rewriter(state: AgentState):\n",
    "    state.update({\"documents\":[], \"on_topic\":\"\", \"rephrased_question\":\"\", \"proceed_to_generate\":False,\n",
    "                  \"rephrase_count\":0, \"needs_gis\":False, \"gis_data\":{}, \"map_html\":\"\"})\n",
    "    state.setdefault(\"messages\", [])\n",
    "    if state[\"question\"] not in state[\"messages\"]:\n",
    "        state[\"messages\"].append(state[\"question\"])\n",
    "    if len(state[\"messages\"])>1:\n",
    "        conv=state[\"messages\"][:-1]\n",
    "        q=state[\"question\"].content\n",
    "        msgs=[SystemMessage(content=\"Rephrase for INGRES retrieval.\")] + conv+[HumanMessage(content=q)]\n",
    "        # Use ChatOpenAI for rewriter to keep a small footprint; adapt depending on available LLM\n",
    "        try:\n",
    "            new_q = ChatOpenAI(model='gpt-4o-mini').invoke(ChatPromptTemplate.from_messages(msgs).format()).content.strip()\n",
    "        except Exception:\n",
    "            # fallback: leave as-is\n",
    "            new_q = q\n",
    "        state[\"rephrased_question\"]=new_q\n",
    "    else:\n",
    "        state[\"rephrased_question\"]=state[\"question\"].content\n",
    "    return state\n",
    "\n",
    "\n",
    "def question_classifier(state: AgentState):\n",
    "    msgs=[SystemMessage(content=\"Is this about INGRES groundwater data? Yes/No\"), HumanMessage(content=state[\"rephrased_question\"]) ]\n",
    "    try:\n",
    "        result = ChatOpenAI(model='gpt-4o-mini').with_structured_output(GradeQuestion).invoke(ChatPromptTemplate.from_messages(msgs).format())\n",
    "        state[\"on_topic\"] = result.score.strip()\n",
    "    except Exception:\n",
    "        # Naive keyword-based fallback\n",
    "        q = state.get('rephrased_question','').lower()\n",
    "        state[\"on_topic\"] = 'yes' if any(k in q for k in ['groundwater','ingres','ground water','water table','gw']) else 'no'\n",
    "    return state\n",
    "\n",
    "\n",
    "def on_topic_router(state: AgentState):\n",
    "    return 'retrieve' if state['on_topic'].lower()=='yes' else 'off_topic_response'\n",
    "\n",
    "\n",
    "def retrieve(state: AgentState):\n",
    "    # Use retriever to get top docs\n",
    "    try:\n",
    "        hits = retriever.get_relevant_documents(state['rephrased_question'])\n",
    "        state['documents'] = hits\n",
    "    except Exception as e:\n",
    "        logger.exception('Retriever failed: %s', e)\n",
    "        state['documents'] = []\n",
    "    return state\n",
    "\n",
    "class GradeDocument(BaseModel):\n",
    "    score: str = Field(description='Relevant? Yes/No')\n",
    "\n",
    "\n",
    "def retrieval_grader(state: AgentState):\n",
    "    relevant = []\n",
    "    for doc in state.get('documents',[]):\n",
    "        try:\n",
    "            msgs=[SystemMessage(content='Relevant to INGRES query?'), HumanMessage(content=f\"{state['rephrased_question']}\\n\\n{doc.page_content}\")]\n",
    "            r = ChatOpenAI(model='gpt-4o-mini').with_structured_output(GradeDocument).invoke(ChatPromptTemplate.from_messages(msgs).format())\n",
    "            if r.score.strip().lower()=='yes':\n",
    "                relevant.append(doc)\n",
    "        except Exception:\n",
    "            # fallback: simple heuristic\n",
    "            if any(tok in doc.page_content.lower() for tok in ['ground','ph','tds','district','year']):\n",
    "                relevant.append(doc)\n",
    "    state['documents'] = relevant\n",
    "    state['proceed_to_generate'] = bool(relevant)\n",
    "    return state\n",
    "\n",
    "\n",
    "def proceed_router(state: AgentState):\n",
    "    if state.get('proceed_to_generate'):\n",
    "        return 'generate_answer'\n",
    "    return 'cannot_answer' if state.get('rephrase_count',0) >= 2 else 'refine_question'\n",
    "\n",
    "\n",
    "def refine_question(state: AgentState):\n",
    "    if state.get('rephrase_count',0) >= 2:\n",
    "        return state\n",
    "    msgs=[SystemMessage(content='Refine INGRES query slightly'), HumanMessage(content=state['rephrased_question'])]\n",
    "    try:\n",
    "        new_q = ChatOpenAI(model='gpt-4o-mini').invoke(ChatPromptTemplate.from_messages(msgs).format()).content.strip()\n",
    "    except Exception:\n",
    "        new_q = state['rephrased_question'] + ' (please be more specific)'\n",
    "    state['rephrased_question'] = new_q\n",
    "    state['rephrase_count'] = state.get('rephrase_count',0) + 1\n",
    "    return state\n",
    "\n",
    "\n",
    "def generate_answer(state: AgentState):\n",
    "    # Use the RAG chain — pass history, context, question\n",
    "    context_text = '\\n\\n---\\n\\n'.join([d.page_content for d in state.get('documents',[])])\n",
    "    history = '\\n'.join([m.content for m in state.get('messages',[])])\n",
    "    try:\n",
    "        res = rag_chain.invoke({'history': history, 'context': context_text, 'question': state['rephrased_question']})\n",
    "        content = res.content.strip()\n",
    "    except Exception as e:\n",
    "        logger.exception('RAG chain failed: %s', e)\n",
    "        # fallback: short summary of docs\n",
    "        content = 'I found the following relevant excerpts:\\n' + '\\n---\\n'.join([d.page_content[:400] for d in state.get('documents',[])])\n",
    "    state.setdefault('messages',[]).append(AIMessage(content=content))\n",
    "    return state\n",
    "\n",
    "\n",
    "def gis_classifier(state: AgentState):\n",
    "    last = state['messages'][-1].content if state.get('messages') else ''\n",
    "    msgs=[SystemMessage(content='Need GIS map? Yes/No'), HumanMessage(content=f\"{state['rephrased_question']}\\n\\n{last}\")]\n",
    "    try:\n",
    "        r = ChatOpenAI(model='gpt-4o-mini').with_structured_output(GISRequest).invoke(ChatPromptTemplate.from_messages(msgs).format())\n",
    "        state['needs_gis'] = r.needs_gis.strip().lower()=='yes'\n",
    "        state['gis_data'] = {'location': r.location_data.strip()} if state['needs_gis'] else {}\n",
    "    except Exception:\n",
    "        # simple heuristic: look for words like 'map', 'location', 'district', 'lat', 'lon'\n",
    "        q = (state.get('rephrased_question','') + ' ' + last).lower()\n",
    "        state['needs_gis'] = any(w in q for w in ['map','location','district','lat','lon','show me','plot'])\n",
    "        state['gis_data'] = {'location': state.get('rephrased_question','')} if state['needs_gis'] else {}\n",
    "    return state\n",
    "\n",
    "\n",
    "def generate_gis_map(state: AgentState):\n",
    "    if not state.get('needs_gis'):\n",
    "        return state\n",
    "    # Very simple sample points. In production you would geocode the extracted location and pull data.\n",
    "    pts = [\n",
    "        {\"lat\":15.3, \"lon\":75.7, \"val\":200},\n",
    "        {\"lat\":19.0, \"lon\":77.0, \"val\":250}\n",
    "    ]\n",
    "    m = folium.Map(location=[22,79], zoom_start=5)\n",
    "    for p in pts:\n",
    "        folium.CircleMarker([p['lat'], p['lon']], radius=5 + p['val']*0.01, popup=str(p['val'])).add_to(m)\n",
    "    state['map_html'] = m._repr_html_()\n",
    "    # append marker in last AI message\n",
    "    last = state['messages'][-1]\n",
    "    state['messages'][-1] = AIMessage(content=last.content + '\\n\\n[GIS Map Attached]')\n",
    "    return state\n",
    "\n",
    "\n",
    "def cannot_answer(state: AgentState):\n",
    "    state.setdefault('messages',[]).append(AIMessage(content=\"I\\'m sorry, I can\\'t find that.\"))\n",
    "    return state\n",
    "\n",
    "\n",
    "def off_topic_response(state: AgentState):\n",
    "    state.setdefault('messages',[]).append(AIMessage(content=\"I\\'m sorry, I cannot answer this.\"))\n",
    "    return state\n",
    "\n",
    "# ---------------------------\n",
    "# Cell 7: Build LangGraph workflow as you specified\n",
    "# ---------------------------\n",
    "cp = MemorySaver()\n",
    "wf = StateGraph(AgentState)\n",
    "\n",
    "# add nodes\n",
    "nodes = [\n",
    "    'question_rewriter','question_classifier','off_topic_response',\n",
    "    'retrieve','retrieval_grader','generate_answer',\n",
    "    'refine_question','cannot_answer',\n",
    "    'gis_classifier','generate_gis_map'\n",
    "]\n",
    "for n in nodes:\n",
    "    wf.add_node(n, globals()[n])\n",
    "\n",
    "# edges following the diagram + GIS after generate_answer\n",
    "wf.set_entry_point('question_rewriter')\n",
    "wf.add_edge('question_rewriter','question_classifier')\n",
    "wf.add_conditional_edges('question_classifier', on_topic_router, {'retrieve':'retrieve','off_topic_response':'off_topic_response'})\n",
    "wf.add_edge('retrieve','retrieval_grader')\n",
    "wf.add_conditional_edges('retrieval_grader', proceed_router, {'generate_answer':'generate_answer','refine_question':'refine_question','cannot_answer':'cannot_answer'})\n",
    "wf.add_edge('refine_question','retrieve')\n",
    "wf.add_edge('generate_answer','gis_classifier')\n",
    "wf.add_conditional_edges('gis_classifier', lambda s: 'generate_gis_map' if s.get('needs_gis') else END, {'generate_gis_map':'generate_gis_map', END:END})\n",
    "wf.add_edge('generate_gis_map', END)\n",
    "wf.add_edge('cannot_answer', END)\n",
    "wf.add_edge('off_topic_response', END)\n",
    "\n",
    "graph = wf.compile(checkpointer=cp)\n",
    "\n",
    "# ---------------------------\n",
    "# Cell 8: Example run function to invoke the graph\n",
    "# ---------------------------\n",
    "\n",
    "def run_query(question_text: str):\n",
    "    initial_state: AgentState = {\n",
    "        'messages': [],\n",
    "        'documents': [],\n",
    "        'on_topic': '',\n",
    "        'rephrased_question': '',\n",
    "        'proceed_to_generate': False,\n",
    "        'rephrase_count': 0,\n",
    "        'question': HumanMessage(content=question_text),\n",
    "        'needs_gis': False,\n",
    "        'gis_data': {},\n",
    "        'map_html': ''\n",
    "    }\n",
    "    state = graph.run(initial_state)\n",
    "    return state\n",
    "\n",
    "# ---------------------------\n",
    "# Cell 9: Try a sample query (uncomment to run)\n",
    "# ---------------------------\n",
    "# result_state = run_query('What is the groundwater extraction rate in Pune district in 2023? Please show a map.')\n",
    "# for msg in result_state['messages']:\n",
    "#     print('\\n---\\n')\n",
    "#     print(msg.content[:2000])\n",
    "# # If map_html present, you can write it to an HTML file to view\n",
    "# if result_state.get('map_html'):\n",
    "#     with open('gis_map.html','w',encoding='utf-8') as f:\n",
    "#         f.write(result_state['map_html'])\n",
    "#     print('Wrote gis_map.html')\n",
    "\n",
    "# ---------------------------\n",
    "# End of notebook script\n",
    "# ---------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "53b0f8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import logging\n",
    "from typing import TypedDict, List\n",
    "\n",
    "import pandas as pd\n",
    "import folium\n",
    "\n",
    "# LangChain / LLM / Embedding\n",
    "from langchain.schema import Document\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Pinecone / LangChain-Pinecone\n",
    "import pinecone\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "# LangGraph\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "df742144",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", api_key=GEMINI_API_KEY)\n",
    "embedding_function = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\", google_api_key=GEMINI_API_KEY)\n",
    "chat_for_classify = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    api_key=os.getenv(\"GEMINI_API_KEY\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b70be0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Initializing Pinecone client...\n",
      "INFO:__main__:Using embedding dimension from function: 768\n",
      "INFO:__main__:Deleting existing Pinecone index sih2\n",
      "INFO:__main__:Creating Pinecone index sih2 with dim=768\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# Cell 3: Pinecone init and vectorstore\n",
    "# ---------------------------\n",
    "import pinecone\n",
    "from pinecone import ServerlessSpec\n",
    "import time # Added this import\n",
    "\n",
    "PINECONE_API_KEY = os.getenv('PINECONE_API_KEY')\n",
    "PINECONE_ENV = os.getenv('PINECONE_ENV')\n",
    "INDEX_NAME = os.getenv('PINECONE_INDEX', 'ingres-index')\n",
    "\n",
    "if not PINECONE_API_KEY:\n",
    "    raise RuntimeError('Please set PINECONE_API_KEY in your environment or .env file')\n",
    "\n",
    "logger.info('Initializing Pinecone client...')\n",
    "pc = pinecone.Pinecone(api_key=PINECONE_API_KEY, environment=PINECONE_ENV)\n",
    "\n",
    "# get embedding dimension from embedding_function\n",
    "try:\n",
    "    example_vec = embedding_function.embed_query('test')\n",
    "    DIM = len(example_vec)\n",
    "    logger.info('Using embedding dimension from function: %s', DIM)\n",
    "except Exception as ex:\n",
    "    logger.warning('Could not get embedding dim from embedding API (%s). Falling back to 384', ex)\n",
    "    DIM = 384\n",
    "\n",
    "# Check if index exists and delete it if it does\n",
    "if INDEX_NAME in pc.list_indexes().names():\n",
    "    logger.info('Deleting existing Pinecone index %s', INDEX_NAME)\n",
    "    pc.delete_index(INDEX_NAME)\n",
    "\n",
    "logger.info('Creating Pinecone index %s with dim=%s', INDEX_NAME, DIM)\n",
    "pc.create_index(\n",
    "    name=INDEX_NAME,\n",
    "    dimension=DIM,\n",
    "    metric='euclidean',\n",
    "    spec=ServerlessSpec(cloud='aws', region='us-east-1')\n",
    ")\n",
    "\n",
    "# Wait for the index to be ready\n",
    "# REMOVED: from pinecone import Index as PineconeIndex\n",
    "# NEW: Use the 'pc' instance to get the index object\n",
    "index = pc.Index(INDEX_NAME)\n",
    "while not index.describe_index_stats().namespaces:\n",
    "    time.sleep(1)\n",
    "\n",
    "# Create langchain_pinecone wrapper\n",
    "vectorstore = PineconeVectorStore(\n",
    "    index=index,\n",
    "    embedding=embedding_function,\n",
    "    pinecone_api_key=PINECONE_API_KEY\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_type='mmr', search_kwargs={'k': 4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5deb1c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    messages: List[BaseMessage]\n",
    "    documents: List[Document]\n",
    "    on_topic: str\n",
    "    rephrased_question: str\n",
    "    proceed_to_generate: bool\n",
    "    rephrase_count: int\n",
    "    question: HumanMessage\n",
    "    needs_gis: bool\n",
    "    gis_data: dict\n",
    "    map_html: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8904d0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradeQuestion(BaseModel):\n",
    "    score: str = Field(description='Is user asking INGRES data? Yes/No')\n",
    "\n",
    "class GISRequest(BaseModel):\n",
    "    needs_gis: str = Field(description='Need GIS map? Yes/No')\n",
    "    location_data: str = Field(description='Extracted location info')\n",
    "\n",
    "class GradeDocument(BaseModel):\n",
    "    score: str = Field(description='Relevant? Yes/No')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "732552db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_rewriter(state: AgentState):\n",
    "    state.update({\"documents\":[], \"on_topic\":\"\", \"rephrased_question\":\"\", \"proceed_to_generate\":False,\n",
    "                  \"rephrase_count\":0, \"needs_gis\":False, \"gis_data\":{}, \"map_html\":\"\"})\n",
    "    state.setdefault(\"messages\", [])\n",
    "    if state[\"question\"] not in state[\"messages\"]:\n",
    "        state[\"messages\"].append(state[\"question\"])\n",
    "    if len(state[\"messages\"]) > 1:\n",
    "        conv = state[\"messages\"][:-1]\n",
    "        q = state[\"question\"].content\n",
    "        msgs = [SystemMessage(content=\"Rephrase for INGRES retrieval.\")] + conv + [HumanMessage(content=q)]\n",
    "        try:\n",
    "            new_q = ChatOpenAI(model='gpt-4o-mini').invoke(ChatPromptTemplate.from_messages(msgs).format()).content.strip()\n",
    "        except Exception:\n",
    "            new_q = q\n",
    "        state[\"rephrased_question\"] = new_q\n",
    "    else:\n",
    "        state[\"rephrased_question\"] = state[\"question\"].content\n",
    "    return state\n",
    "\n",
    "def question_classifier(state: AgentState):\n",
    "    msgs = [SystemMessage(content=\"Is this about INGRES groundwater data? Yes/No\"), HumanMessage(content=state[\"rephrased_question\"])]\n",
    "    try:\n",
    "        result = ChatOpenAI(model='gpt-4o-mini').with_structured_output(GradeQuestion).invoke(ChatPromptTemplate.from_messages(msgs).format())\n",
    "        state[\"on_topic\"] = result.score.strip()\n",
    "    except Exception:\n",
    "        q = state.get('rephrased_question','').lower()\n",
    "        state[\"on_topic\"] = 'yes' if any(k in q for k in ['groundwater', 'ingres', 'ground water', 'water table', 'gw']) else 'no'\n",
    "    return state\n",
    "\n",
    "def on_topic_router(state: AgentState):\n",
    "    return 'retrieve' if state['on_topic'].lower() == 'yes' else 'off_topic_response'\n",
    "\n",
    "def retrieve(state: AgentState):\n",
    "    try:\n",
    "        hits = retriever.get_relevant_documents(state['rephrased_question'])\n",
    "        state['documents'] = hits\n",
    "    except Exception as e:\n",
    "        logger.exception('Retriever failed: %s', e)\n",
    "        state['documents'] = []\n",
    "    return state\n",
    "\n",
    "def retrieval_grader(state: AgentState):\n",
    "    relevant = []\n",
    "    for doc in state.get('documents',[]):\n",
    "        try:\n",
    "            msgs = [SystemMessage(content='Relevant to INGRES query?'), HumanMessage(content=f\"{state['rephrased_question']}\\n\\n{doc.page_content}\")]\n",
    "            r = ChatOpenAI(model='gpt-4o-mini').with_structured_output(GradeDocument).invoke(ChatPromptTemplate.from_messages(msgs).format())\n",
    "            if r.score.strip().lower() == 'yes':\n",
    "                relevant.append(doc)\n",
    "        except Exception:\n",
    "            if any(tok in doc.page_content.lower() for tok in ['ground','ph','tds','district','year']):\n",
    "                relevant.append(doc)\n",
    "    state['documents'] = relevant\n",
    "    state['proceed_to_generate'] = bool(relevant)\n",
    "    return state\n",
    "\n",
    "def proceed_router(state: AgentState):\n",
    "    if state.get('proceed_to_generate'):\n",
    "        return 'generate_answer'\n",
    "    return 'cannot_answer' if state.get('rephrase_count',0) >= 2 else 'refine_question'\n",
    "\n",
    "def refine_question(state: AgentState):\n",
    "    if state.get('rephrase_count',0) >= 2:\n",
    "        return state\n",
    "    msgs = [SystemMessage(content='Refine INGRES query slightly'), HumanMessage(content=state['rephrased_question'])]\n",
    "    try:\n",
    "        new_q = ChatOpenAI(model='gpt-4o-mini').invoke(ChatPromptTemplate.from_messages(msgs).format()).content.strip()\n",
    "    except Exception:\n",
    "        new_q = state['rephrased_question'] + ' (please be more specific)'\n",
    "    state['rephrased_question'] = new_q\n",
    "    state['rephrase_count'] = state.get('rephrase_count',0) + 1\n",
    "    return state\n",
    "\n",
    "def generate_answer(state: AgentState):\n",
    "    context_text = '\\n\\n---\\n\\n'.join([d.page_content for d in state.get('documents',[])])\n",
    "    history = '\\n'.join([m.content for m in state.get('messages',[])])\n",
    "    try:\n",
    "        res = ChatOpenAI(model='gpt-4o-mini').invoke({'history': history, 'context': context_text, 'question': state['rephrased_question']})\n",
    "        content = res.content.strip()\n",
    "    except Exception as e:\n",
    "        logger.exception('RAG chain failed: %s', e)\n",
    "        content = 'I found the following relevant excerpts:\\n' + '\\n---\\n'.join([d.page_content[:400] for d in state.get('documents',[])])\n",
    "    state.setdefault('messages',[]).append(AIMessage(content=content))\n",
    "    return state\n",
    "\n",
    "def gis_classifier(state: AgentState):\n",
    "    last = state['messages'][-1].content if state.get('messages') else ''\n",
    "    msgs = [SystemMessage(content='Need GIS map? Yes/No'), HumanMessage(content=f\"{state['rephrased_question']}\\n\\n{last}\")]\n",
    "    try:\n",
    "        r = ChatOpenAI(model='gpt-4o-mini').with_structured_output(GISRequest).invoke(ChatPromptTemplate.from_messages(msgs).format())\n",
    "        state['needs_gis'] = r.needs_gis.strip().lower() == 'yes'\n",
    "        state['gis_data'] = {'location': r.location_data.strip()} if state['needs_gis'] else {}\n",
    "    except Exception:\n",
    "        q = (state.get('rephrased_question','') + ' ' + last).lower()\n",
    "        state['needs_gis'] = any(w in q for w in ['map','location','district','lat','lon','show me','plot'])\n",
    "        state['gis_data'] = {'location': state.get('rephrased_question','')} if state['needs_gis'] else {}\n",
    "    return state\n",
    "\n",
    "def generate_gis_map(state: AgentState):\n",
    "    if not state.get('needs_gis'):\n",
    "        return state\n",
    "    pts = [\n",
    "        {\"lat\":15.3, \"lon\":75.7, \"val\":200},\n",
    "        {\"lat\":19.0, \"lon\":77.0, \"val\":250}\n",
    "    ]\n",
    "    m = folium.Map(location=[22,79], zoom_start=5)\n",
    "    for p in pts:\n",
    "        folium.CircleMarker([p['lat'], p['lon']], radius=5 + p['val']*0.01, popup=str(p['val'])).add_to(m)\n",
    "    state['map_html'] = m._repr_html_()\n",
    "    last = state['messages'][-1]\n",
    "    state['messages'][-1] = AIMessage(content=last.content + '\\n\\n[GIS Map Attached]')\n",
    "    return state\n",
    "\n",
    "def cannot_answer(state: AgentState):\n",
    "    state.setdefault('messages',[]).append(AIMessage(content=\"I'm sorry, I can't find that.\"))\n",
    "    return state\n",
    "\n",
    "def off_topic_response(state: AgentState):\n",
    "    state.setdefault('messages',[]).append(AIMessage(content=\"I'm sorry, I cannot answer this.\"))\n",
    "    return state\n",
    "\n",
    "# ---------------------------\n",
    "# Build LangGraph workflow\n",
    "# ---------------------------\n",
    "cp = MemorySaver()\n",
    "wf = StateGraph(AgentState)\n",
    "\n",
    "nodes = [\n",
    "    'question_rewriter','question_classifier','off_topic_response',\n",
    "    'retrieve','retrieval_grader','generate_answer',\n",
    "    'refine_question','cannot_answer',\n",
    "    'gis_classifier','generate_gis_map'\n",
    "]\n",
    "for n in nodes:\n",
    "    wf.add_node(n, globals()[n])\n",
    "\n",
    "wf.set_entry_point('question_rewriter')\n",
    "wf.add_edge('question_rewriter','question_classifier')\n",
    "wf.add_conditional_edges('question_classifier', on_topic_router, {'retrieve':'retrieve','off_topic_response':'off_topic_response'})\n",
    "wf.add_edge('retrieve','retrieval_grader')\n",
    "wf.add_conditional_edges('retrieval_grader', proceed_router, {'generate_answer':'generate_answer','refine_question':'refine_question','cannot_answer':'cannot_answer'})\n",
    "wf.add_edge('refine_question','retrieve')\n",
    "wf.add_edge('generate_answer','gis_classifier')\n",
    "wf.add_conditional_edges('gis_classifier', lambda s: 'generate_gis_map' if s.get('needs_gis') else END, {'generate_gis_map':'generate_gis_map', END:END})\n",
    "wf.add_edge('generate_gis_map', END)\n",
    "wf.add_edge('cannot_answer', END)\n",
    "wf.add_edge('off_topic_response', END)\n",
    "\n",
    "graph = wf.compile(checkpointer=cp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21bfe3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Retriever failed: (400)\n",
      "Reason: Bad Request\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Sat, 06 Sep 2025 13:51:35 GMT', 'Content-Type': 'application/json', 'Content-Length': '102', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '406', 'x-pinecone-request-id': '8036904116552971277', 'x-envoy-upstream-service-time': '30', 'server': 'envoy'})\n",
      "HTTP response body: {\"code\":3,\"message\":\"Vector dimension 768 does not match the dimension of the index 384\",\"details\":[]}\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\prita\\AppData\\Local\\Temp\\ipykernel_20396\\1266092739.py\", line 35, in retrieve\n",
      "    hits = retriever.get_relevant_documents(state['rephrased_question'])\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\prita\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain_core\\_api\\deprecation.py\", line 190, in warning_emitting_wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\prita\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain_core\\retrievers.py\", line 414, in get_relevant_documents\n",
      "    return self.invoke(query, config, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\prita\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain_core\\retrievers.py\", line 261, in invoke\n",
      "    result = self._get_relevant_documents(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\prita\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain_core\\vectorstores\\base.py\", line 1089, in _get_relevant_documents\n",
      "    docs = self.vectorstore.max_marginal_relevance_search(query, **kwargs_)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\prita\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain_pinecone\\vectorstores.py\", line 807, in max_marginal_relevance_search\n",
      "    return self.max_marginal_relevance_search_by_vector(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\prita\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain_pinecone\\vectorstores.py\", line 699, in max_marginal_relevance_search_by_vector\n",
      "    results = self.index.query(\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\prita\\AppData\\Roaming\\Python\\Python311\\site-packages\\pinecone\\utils\\error_handling.py\", line 30, in inner_func\n",
      "    raise e from e\n",
      "  File \"C:\\Users\\prita\\AppData\\Roaming\\Python\\Python311\\site-packages\\pinecone\\utils\\error_handling.py\", line 15, in inner_func\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\prita\\AppData\\Roaming\\Python\\Python311\\site-packages\\pinecone\\db_data\\index.py\", line 345, in query\n",
      "    response = self._query(\n",
      "               ^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\prita\\AppData\\Roaming\\Python\\Python311\\site-packages\\pinecone\\db_data\\index.py\", line 395, in _query\n",
      "    return self._vector_api.query_vectors(request, **self._openapi_kwargs(kwargs))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\prita\\AppData\\Roaming\\Python\\Python311\\site-packages\\pinecone\\openapi_support\\endpoint.py\", line 102, in __call__\n",
      "    return self.callable(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\prita\\AppData\\Roaming\\Python\\Python311\\site-packages\\pinecone\\core\\openapi\\db_data\\api\\vector_operations_api.py\", line 388, in __query_vectors\n",
      "    return self.call_with_http_info(**kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\prita\\AppData\\Roaming\\Python\\Python311\\site-packages\\pinecone\\openapi_support\\endpoint.py\", line 134, in call_with_http_info\n",
      "    return self.api_client.call_api(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\prita\\AppData\\Roaming\\Python\\Python311\\site-packages\\pinecone\\openapi_support\\api_client.py\", line 306, in call_api\n",
      "    return self.__call_api(\n",
      "           ^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\prita\\AppData\\Roaming\\Python\\Python311\\site-packages\\pinecone\\openapi_support\\api_client.py\", line 182, in __call_api\n",
      "    raise e\n",
      "  File \"C:\\Users\\prita\\AppData\\Roaming\\Python\\Python311\\site-packages\\pinecone\\openapi_support\\api_client.py\", line 170, in __call_api\n",
      "    response_data = self.request(\n",
      "                    ^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\prita\\AppData\\Roaming\\Python\\Python311\\site-packages\\pinecone\\openapi_support\\api_client.py\", line 386, in request\n",
      "    return self.rest_client.POST(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\prita\\AppData\\Roaming\\Python\\Python311\\site-packages\\pinecone\\openapi_support\\rest_utils.py\", line 146, in POST\n",
      "    return self.request(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\prita\\AppData\\Roaming\\Python\\Python311\\site-packages\\pinecone\\openapi_support\\rest_urllib3.py\", line 267, in request\n",
      "    return raise_exceptions_or_return(r)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\prita\\AppData\\Roaming\\Python\\Python311\\site-packages\\pinecone\\openapi_support\\rest_utils.py\", line 49, in raise_exceptions_or_return\n",
      "    raise PineconeApiException(http_resp=r)\n",
      "pinecone.exceptions.exceptions.PineconeApiException: (400)\n",
      "Reason: Bad Request\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Sat, 06 Sep 2025 13:51:35 GMT', 'Content-Type': 'application/json', 'Content-Length': '102', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '406', 'x-pinecone-request-id': '8036904116552971277', 'x-envoy-upstream-service-time': '30', 'server': 'envoy'})\n",
      "HTTP response body: {\"code\":3,\"message\":\"Vector dimension 768 does not match the dimension of the index 384\",\"details\":[]}\n",
      "\n",
      "ERROR:__main__:Retriever failed: (400)\n",
      "Reason: Bad Request\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Sat, 06 Sep 2025 13:51:36 GMT', 'Content-Type': 'application/json', 'Content-Length': '102', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '3', 'x-pinecone-request-id': '8994839571665634649', 'x-envoy-upstream-service-time': '0', 'server': 'envoy'})\n",
      "HTTP response body: {\"code\":3,\"message\":\"Vector dimension 768 does not match the dimension of the index 384\",\"details\":[]}\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\prita\\AppData\\Local\\Temp\\ipykernel_20396\\1266092739.py\", line 35, in retrieve\n",
      "    hits = retriever.get_relevant_documents(state['rephrased_question'])\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\prita\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain_core\\_api\\deprecation.py\", line 190, in warning_emitting_wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\prita\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain_core\\retrievers.py\", line 414, in get_relevant_documents\n",
      "    return self.invoke(query, config, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\prita\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain_core\\retrievers.py\", line 261, in invoke\n",
      "    result = self._get_relevant_documents(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\prita\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain_core\\vectorstores\\base.py\", line 1089, in _get_relevant_documents\n",
      "    docs = self.vectorstore.max_marginal_relevance_search(query, **kwargs_)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\prita\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain_pinecone\\vectorstores.py\", line 807, in max_marginal_relevance_search\n",
      "    return self.max_marginal_relevance_search_by_vector(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\prita\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain_pinecone\\vectorstores.py\", line 699, in max_marginal_relevance_search_by_vector\n",
      "    results = self.index.query(\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\prita\\AppData\\Roaming\\Python\\Python311\\site-packages\\pinecone\\utils\\error_handling.py\", line 30, in inner_func\n",
      "    raise e from e\n",
      "  File \"C:\\Users\\prita\\AppData\\Roaming\\Python\\Python311\\site-packages\\pinecone\\utils\\error_handling.py\", line 15, in inner_func\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\prita\\AppData\\Roaming\\Python\\Python311\\site-packages\\pinecone\\db_data\\index.py\", line 345, in query\n",
      "    response = self._query(\n",
      "               ^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\prita\\AppData\\Roaming\\Python\\Python311\\site-packages\\pinecone\\db_data\\index.py\", line 395, in _query\n",
      "    return self._vector_api.query_vectors(request, **self._openapi_kwargs(kwargs))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\prita\\AppData\\Roaming\\Python\\Python311\\site-packages\\pinecone\\openapi_support\\endpoint.py\", line 102, in __call__\n",
      "    return self.callable(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\prita\\AppData\\Roaming\\Python\\Python311\\site-packages\\pinecone\\core\\openapi\\db_data\\api\\vector_operations_api.py\", line 388, in __query_vectors\n",
      "    return self.call_with_http_info(**kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\prita\\AppData\\Roaming\\Python\\Python311\\site-packages\\pinecone\\openapi_support\\endpoint.py\", line 134, in call_with_http_info\n",
      "    return self.api_client.call_api(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\prita\\AppData\\Roaming\\Python\\Python311\\site-packages\\pinecone\\openapi_support\\api_client.py\", line 306, in call_api\n",
      "    return self.__call_api(\n",
      "           ^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\prita\\AppData\\Roaming\\Python\\Python311\\site-packages\\pinecone\\openapi_support\\api_client.py\", line 182, in __call_api\n",
      "    raise e\n",
      "  File \"C:\\Users\\prita\\AppData\\Roaming\\Python\\Python311\\site-packages\\pinecone\\openapi_support\\api_client.py\", line 170, in __call_api\n",
      "    response_data = self.request(\n",
      "                    ^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\prita\\AppData\\Roaming\\Python\\Python311\\site-packages\\pinecone\\openapi_support\\api_client.py\", line 386, in request\n",
      "    return self.rest_client.POST(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\prita\\AppData\\Roaming\\Python\\Python311\\site-packages\\pinecone\\openapi_support\\rest_utils.py\", line 146, in POST\n",
      "    return self.request(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\prita\\AppData\\Roaming\\Python\\Python311\\site-packages\\pinecone\\openapi_support\\rest_urllib3.py\", line 267, in request\n",
      "    return raise_exceptions_or_return(r)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\prita\\AppData\\Roaming\\Python\\Python311\\site-packages\\pinecone\\openapi_support\\rest_utils.py\", line 49, in raise_exceptions_or_return\n",
      "    raise PineconeApiException(http_resp=r)\n",
      "pinecone.exceptions.exceptions.PineconeApiException: (400)\n",
      "Reason: Bad Request\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Sat, 06 Sep 2025 13:51:36 GMT', 'Content-Type': 'application/json', 'Content-Length': '102', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '3', 'x-pinecone-request-id': '8994839571665634649', 'x-envoy-upstream-service-time': '0', 'server': 'envoy'})\n",
      "HTTP response body: {\"code\":3,\"message\":\"Vector dimension 768 does not match the dimension of the index 384\",\"details\":[]}\n",
      "\n",
      "ERROR:__main__:Retriever failed: (400)\n",
      "Reason: Bad Request\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Sat, 06 Sep 2025 13:51:37 GMT', 'Content-Type': 'application/json', 'Content-Length': '102', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '41', 'x-pinecone-request-id': '7773196901710863120', 'x-envoy-upstream-service-time': '39', 'server': 'envoy'})\n",
      "HTTP response body: {\"code\":3,\"message\":\"Vector dimension 768 does not match the dimension of the index 384\",\"details\":[]}\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\prita\\AppData\\Local\\Temp\\ipykernel_20396\\1266092739.py\", line 35, in retrieve\n",
      "    hits = retriever.get_relevant_documents(state['rephrased_question'])\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\prita\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain_core\\_api\\deprecation.py\", line 190, in warning_emitting_wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\prita\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain_core\\retrievers.py\", line 414, in get_relevant_documents\n",
      "    return self.invoke(query, config, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\prita\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain_core\\retrievers.py\", line 261, in invoke\n",
      "    result = self._get_relevant_documents(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\prita\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain_core\\vectorstores\\base.py\", line 1089, in _get_relevant_documents\n",
      "    docs = self.vectorstore.max_marginal_relevance_search(query, **kwargs_)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\prita\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain_pinecone\\vectorstores.py\", line 807, in max_marginal_relevance_search\n",
      "    return self.max_marginal_relevance_search_by_vector(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\prita\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain_pinecone\\vectorstores.py\", line 699, in max_marginal_relevance_search_by_vector\n",
      "    results = self.index.query(\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\prita\\AppData\\Roaming\\Python\\Python311\\site-packages\\pinecone\\utils\\error_handling.py\", line 30, in inner_func\n",
      "    raise e from e\n",
      "  File \"C:\\Users\\prita\\AppData\\Roaming\\Python\\Python311\\site-packages\\pinecone\\utils\\error_handling.py\", line 15, in inner_func\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\prita\\AppData\\Roaming\\Python\\Python311\\site-packages\\pinecone\\db_data\\index.py\", line 345, in query\n",
      "    response = self._query(\n",
      "               ^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\prita\\AppData\\Roaming\\Python\\Python311\\site-packages\\pinecone\\db_data\\index.py\", line 395, in _query\n",
      "    return self._vector_api.query_vectors(request, **self._openapi_kwargs(kwargs))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\prita\\AppData\\Roaming\\Python\\Python311\\site-packages\\pinecone\\openapi_support\\endpoint.py\", line 102, in __call__\n",
      "    return self.callable(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\prita\\AppData\\Roaming\\Python\\Python311\\site-packages\\pinecone\\core\\openapi\\db_data\\api\\vector_operations_api.py\", line 388, in __query_vectors\n",
      "    return self.call_with_http_info(**kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\prita\\AppData\\Roaming\\Python\\Python311\\site-packages\\pinecone\\openapi_support\\endpoint.py\", line 134, in call_with_http_info\n",
      "    return self.api_client.call_api(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\prita\\AppData\\Roaming\\Python\\Python311\\site-packages\\pinecone\\openapi_support\\api_client.py\", line 306, in call_api\n",
      "    return self.__call_api(\n",
      "           ^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\prita\\AppData\\Roaming\\Python\\Python311\\site-packages\\pinecone\\openapi_support\\api_client.py\", line 182, in __call_api\n",
      "    raise e\n",
      "  File \"C:\\Users\\prita\\AppData\\Roaming\\Python\\Python311\\site-packages\\pinecone\\openapi_support\\api_client.py\", line 170, in __call_api\n",
      "    response_data = self.request(\n",
      "                    ^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\prita\\AppData\\Roaming\\Python\\Python311\\site-packages\\pinecone\\openapi_support\\api_client.py\", line 386, in request\n",
      "    return self.rest_client.POST(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\prita\\AppData\\Roaming\\Python\\Python311\\site-packages\\pinecone\\openapi_support\\rest_utils.py\", line 146, in POST\n",
      "    return self.request(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\prita\\AppData\\Roaming\\Python\\Python311\\site-packages\\pinecone\\openapi_support\\rest_urllib3.py\", line 267, in request\n",
      "    return raise_exceptions_or_return(r)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\prita\\AppData\\Roaming\\Python\\Python311\\site-packages\\pinecone\\openapi_support\\rest_utils.py\", line 49, in raise_exceptions_or_return\n",
      "    raise PineconeApiException(http_resp=r)\n",
      "pinecone.exceptions.exceptions.PineconeApiException: (400)\n",
      "Reason: Bad Request\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Sat, 06 Sep 2025 13:51:37 GMT', 'Content-Type': 'application/json', 'Content-Length': '102', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '41', 'x-pinecone-request-id': '7773196901710863120', 'x-envoy-upstream-service-time': '39', 'server': 'envoy'})\n",
      "HTTP response body: {\"code\":3,\"message\":\"Vector dimension 768 does not match the dimension of the index 384\",\"details\":[]}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: I'm sorry, I can't find that.\n",
      "Goodbye!\n"
     ]
    }
   ],
   "source": [
    "# To use this code, run your full script first to compile the graph.\n",
    "# Then you can run the following lines in your environment.\n",
    "\n",
    "# Create an interactive loop to chat with the graph\n",
    "# For a single session, you can use a fixed thread_id.\n",
    "# For a real application, you would generate a unique ID per user.\n",
    "thread_id = \"my-ingres-chatbot-session-1\"\n",
    "\n",
    "while True:\n",
    "    user_query = input(\"You: \")\n",
    "    if user_query.lower() in [\"exit\", \"quit\", \"q\"]:\n",
    "        print(\"Goodbye!\")\n",
    "        break\n",
    "\n",
    "    try:\n",
    "        # Pass the 'configurable' dictionary with the thread_id\n",
    "        final_state = graph.invoke(\n",
    "            {\"question\": HumanMessage(content=user_query)},\n",
    "            config={\"configurable\": {\"thread_id\": thread_id}}\n",
    "        )\n",
    "        \n",
    "        last_message = final_state['messages'][-1]\n",
    "        print(f\"Chatbot: {last_message.content}\")\n",
    "        \n",
    "        if final_state.get('map_html'):\n",
    "            print(\"\\n[GIS map generated]\\n\")\n",
    "            with open(\"gis_map.html\", \"w\") as f:\n",
    "                f.write(final_state['map_html'])\n",
    "            print(\"Map saved as gis_map.html. Open it in your browser to view.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e36a65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
