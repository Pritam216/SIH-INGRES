{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "53b0f8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import logging\n",
    "from typing import TypedDict, List\n",
    "\n",
    "import pandas as pd\n",
    "import folium\n",
    "\n",
    "# LangChain / LLM / Embedding\n",
    "from langchain.schema import Document\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Pinecone / LangChain-Pinecone\n",
    "import pinecone\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "# LangGraph\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "df742144",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "embedding_function = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\", google_api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "chat_for_classify = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    api_key=os.getenv(\"GEMINI_API_KEY\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9235ea5f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Pinecone' from 'pinecone' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlogging\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdotenv\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dotenv\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpinecone\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Pinecone, ServerlessSpec\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtext_splitter\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RecursiveCharacterTextSplitter\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_pinecone\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PineconeVectorStore\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'Pinecone' from 'pinecone' (unknown location)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import gc\n",
    "import time\n",
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from pinecone import pinecone, ServerlessSpec\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "# ---------------------------\n",
    "# Logging setup\n",
    "# ---------------------------\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "# ---------------------------\n",
    "# Load environment variables\n",
    "# ---------------------------\n",
    "load_dotenv()\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "PINECONE_INDEX_NAME = os.getenv(\"PINECONE_INDEX\", \"ingres-index\")\n",
    "\n",
    "if not PINECONE_API_KEY:\n",
    "    raise RuntimeError(\"Please set PINECONE_API_KEY in your environment or .env file\")\n",
    "\n",
    "# ---------------------------\n",
    "# 1. Load Data from CSVs\n",
    "# ---------------------------\n",
    "try:\n",
    "    df1 = pd.read_csv(\"data/dwlr_realtime_data.csv\")\n",
    "    df2 = pd.read_csv(\"data/groundwater_quality_data.csv\")\n",
    "    df3 = pd.read_csv(\"data/groundwater_trends_2015_2023.csv\")\n",
    "    df4 = pd.read_csv(\"data/ingres_assessment_units_2023.csv\")\n",
    "    logging.info(\"Successfully loaded CSV files.\")\n",
    "except FileNotFoundError as e:\n",
    "    logging.error(f\"Error loading CSV file: {e}. Please ensure 'data/' folder exists with CSV files.\")\n",
    "    exit()\n",
    "\n",
    "# ---------------------------\n",
    "# 2. Convert DataFrames â†’ Text Documents\n",
    "# ---------------------------\n",
    "def df_to_text(df, name):\n",
    "    docs = []\n",
    "    for _, row in df.iterrows():\n",
    "        text = f\"{name} Record: \" + \" | \".join([f\"{col}: {row[col]}\" for col in df.columns if pd.notna(row[col])])\n",
    "        docs.append(text)\n",
    "    return docs\n",
    "\n",
    "docs1 = df_to_text(df1, \"Realtime\")\n",
    "docs2 = df_to_text(df2, \"Quality\")\n",
    "docs3 = df_to_text(df3, \"Trend\")\n",
    "docs4 = df_to_text(df4, \"Assessment\")\n",
    "all_docs = docs1 + docs2 + docs3 + docs4\n",
    "gc.collect()\n",
    "\n",
    "# ---------------------------\n",
    "# 3. Chunk Documents\n",
    "# ---------------------------\n",
    "logging.info(\"Chunking documents...\")\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "documents = splitter.create_documents(all_docs)\n",
    "logging.info(f\"Created {len(documents)} chunks.\")\n",
    "\n",
    "# ---------------------------\n",
    "# 4. Initialize Embeddings\n",
    "# ---------------------------\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "if not GEMINI_API_KEY:\n",
    "    raise RuntimeError(\"Please set GEMINI_API_KEY in your environment or .env file\")\n",
    "\n",
    "embedding_function = GoogleGenerativeAIEmbeddings(\n",
    "    model=\"models/embedding-001\", \n",
    "    google_api_key=GEMINI_API_KEY\n",
    ")\n",
    "\n",
    "# Test embedding to get dimension\n",
    "test_vector = embedding_function.embed_query(\"hello world\")\n",
    "DIMENSION = len(test_vector)\n",
    "logging.info(f\"Embedding dimension: {DIMENSION}\")\n",
    "\n",
    "# ---------------------------\n",
    "# 5. Initialize Pinecone client\n",
    "# ---------------------------\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "\n",
    "# Get existing indexes\n",
    "existing_indexes = [idx.name for idx in pc.list_indexes()]\n",
    "\n",
    "if PINECONE_INDEX_NAME not in existing_indexes:\n",
    "    logging.info(f\"Creating Pinecone index '{PINECONE_INDEX_NAME}' with dim={DIMENSION}...\")\n",
    "    pc.create_index(\n",
    "        name=PINECONE_INDEX_NAME,\n",
    "        dimension=DIMENSION,\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n",
    "    )\n",
    "    # Wait for index to be ready\n",
    "    while True:\n",
    "        idx_list = pc.list_indexes()\n",
    "        if any(idx.name == PINECONE_INDEX_NAME for idx in idx_list):\n",
    "            break\n",
    "        logging.info(\"Waiting for index to be ready...\")\n",
    "        time.sleep(2)\n",
    "    logging.info(f\"Index '{PINECONE_INDEX_NAME}' is ready.\")\n",
    "else:\n",
    "    logging.info(f\"Index '{PINECONE_INDEX_NAME}' already exists. Connecting...\")\n",
    "\n",
    "# ---------------------------\n",
    "# 6. Create VectorStore & Upsert\n",
    "# ---------------------------\n",
    "logging.info(\"Upserting documents into Pinecone...\")\n",
    "vectorstore = PineconeVectorStore.from_documents(\n",
    "    documents=documents,\n",
    "    embedding=embedding_function,\n",
    "    index_name=PINECONE_INDEX_NAME\n",
    ")\n",
    "logging.info(\"Documents successfully upserted.\")\n",
    "\n",
    "# ---------------------------\n",
    "# 7. Create Retriever\n",
    "# ---------------------------\n",
    "retriever = vectorstore.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 4})\n",
    "logging.info(\"Retriever is ready.\")\n",
    "\n",
    "print(\"\\n--- Process Complete ---\")\n",
    "print(f\"Pinecone index '{PINECONE_INDEX_NAME}' contains your embeddings.\")\n",
    "print(\"The 'retriever' object is ready for use in your RAG pipeline.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b70be0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'Pinecone'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdotenv\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dotenv\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mPinecone\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Pinecone, ServerlessSpec\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_google_genai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GoogleGenerativeAIEmbeddings\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Load API keys\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'Pinecone'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from Pinecone import pinecone, ServerlessSpec\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "# -----------------------------\n",
    "# Load API keys\n",
    "# -----------------------------\n",
    "load_dotenv()\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "# -----------------------------\n",
    "# Setup embeddings\n",
    "# -----------------------------\n",
    "embeddings = GoogleGenerativeAIEmbeddings(\n",
    "    model=\"models/embedding-001\",\n",
    "    google_api_key=GEMINI_API_KEY\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Setup Pinecone\n",
    "# -----------------------------\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "index_name = \"groundwater-index\"\n",
    "\n",
    "# Create index if not exists\n",
    "if index_name not in [i.name for i in pc.list_indexes()]:\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=768,  # Gemini embedding dimension\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n",
    "    )\n",
    "\n",
    "index = pc.Index(index_name)\n",
    "\n",
    "# -----------------------------\n",
    "# Load CSVs and prepare text\n",
    "# -----------------------------\n",
    "folder = \"data\"\n",
    "files = [\n",
    "    \"dwlr_realtime_data.csv\",\n",
    "    \"groundwater_quality_data.csv\",\n",
    "    \"groundwater_trends_2015_2023.csv\",\n",
    "    \"ingres_assessment_units_2023.csv\"\n",
    "]\n",
    "\n",
    "all_texts = []\n",
    "for file in files:\n",
    "    df = pd.read_csv(os.path.join(folder, file))\n",
    "    for i, row in df.iterrows():\n",
    "        text = \" | \".join([f\"{col}: {row[col]}\" for col in df.columns])\n",
    "        all_texts.append((f\"{file}_{i}\", text, {\"source\": file}))\n",
    "\n",
    "# -----------------------------\n",
    "# Generate embeddings & upsert\n",
    "# -----------------------------\n",
    "vectors = []\n",
    "for _id, text, meta in all_texts:\n",
    "    vector = embeddings.embed_query(text)\n",
    "    vectors.append((_id, vector, meta))\n",
    "\n",
    "# Batch insert (Pinecone prefers batches)\n",
    "index.upsert(vectors[:100])  # you can loop in chunks if too large\n",
    "\n",
    "print(f\"Inserted {len(vectors)} vectors into Pinecone!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5deb1c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    messages: List[BaseMessage]\n",
    "    documents: List[Document]\n",
    "    on_topic: str\n",
    "    rephrased_question: str\n",
    "    proceed_to_generate: bool\n",
    "    rephrase_count: int\n",
    "    question: HumanMessage\n",
    "    needs_gis: bool\n",
    "    gis_data: dict\n",
    "    map_html: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8904d0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradeQuestion(BaseModel):\n",
    "    score: str = Field(description='Is user asking INGRES data? Yes/No')\n",
    "\n",
    "class GISRequest(BaseModel):\n",
    "    needs_gis: str = Field(description='Need GIS map? Yes/No')\n",
    "    location_data: str = Field(description='Extracted location info')\n",
    "\n",
    "class GradeDocument(BaseModel):\n",
    "    score: str = Field(description='Relevant? Yes/No')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732552db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_rewriter(state: AgentState):\n",
    "    state.update({\"documents\":[], \"on_topic\":\"\", \"rephrased_question\":\"\", \"proceed_to_generate\":False,\n",
    "                  \"rephrase_count\":0, \"needs_gis\":False, \"gis_data\":{}, \"map_html\":\"\"})\n",
    "    state.setdefault(\"messages\", [])\n",
    "    if state[\"question\"] not in state[\"messages\"]:\n",
    "        state[\"messages\"].append(state[\"question\"])\n",
    "    if len(state[\"messages\"]) > 1:\n",
    "        conv = state[\"messages\"][:-1]\n",
    "        q = state[\"question\"].content\n",
    "        msgs = [SystemMessage(content=\"Rephrase for INGRES retrieval.\")] + conv + [HumanMessage(content=q)]\n",
    "        try:\n",
    "            new_q = ChatOpenAI(model='gpt-4o-mini').invoke(ChatPromptTemplate.from_messages(msgs).format()).content.strip()\n",
    "        except Exception:\n",
    "            new_q = q\n",
    "        state[\"rephrased_question\"] = new_q\n",
    "    else:\n",
    "        state[\"rephrased_question\"] = state[\"question\"].content\n",
    "    return state\n",
    "\n",
    "def question_classifier(state: AgentState):\n",
    "    msgs = [SystemMessage(content=\"Is this about INGRES groundwater data? Yes/No\"), HumanMessage(content=state[\"rephrased_question\"])]\n",
    "    try:\n",
    "        result = ChatOpenAI(model='gpt-4o-mini').with_structured_output(GradeQuestion).invoke(ChatPromptTemplate.from_messages(msgs).format())\n",
    "        state[\"on_topic\"] = result.score.strip()\n",
    "    except Exception:\n",
    "        q = state.get('rephrased_question','').lower()\n",
    "        state[\"on_topic\"] = 'yes' if any(k in q for k in ['groundwater', 'ingres', 'ground water', 'water table', 'gw']) else 'no'\n",
    "    return state\n",
    "\n",
    "def on_topic_router(state: AgentState):\n",
    "    return 'retrieve' if state['on_topic'].lower() == 'yes' else 'off_topic_response'\n",
    "\n",
    "def retrieve(state: AgentState):\n",
    "    try:\n",
    "        hits = retriever.get_relevant_documents(state['rephrased_question'])\n",
    "        state['documents'] = hits\n",
    "    except Exception as e:\n",
    "        logger.exception('Retriever failed: %s', e)\n",
    "        state['documents'] = []\n",
    "    return state\n",
    "\n",
    "def retrieval_grader(state: AgentState):\n",
    "    relevant = []\n",
    "    for doc in state.get('documents',[]):\n",
    "        try:\n",
    "            msgs = [SystemMessage(content='Relevant to INGRES query?'), HumanMessage(content=f\"{state['rephrased_question']}\\n\\n{doc.page_content}\")]\n",
    "            r = ChatOpenAI(model='gpt-4o-mini').with_structured_output(GradeDocument).invoke(ChatPromptTemplate.from_messages(msgs).format())\n",
    "            if r.score.strip().lower() == 'yes':\n",
    "                relevant.append(doc)\n",
    "        except Exception:\n",
    "            if any(tok in doc.page_content.lower() for tok in ['ground','ph','tds','district','year']):\n",
    "                relevant.append(doc)\n",
    "    state['documents'] = relevant\n",
    "    state['proceed_to_generate'] = bool(relevant)\n",
    "    return state\n",
    "\n",
    "def proceed_router(state: AgentState):\n",
    "    if state.get('proceed_to_generate'):\n",
    "        return 'generate_answer'\n",
    "    return 'cannot_answer' if state.get('rephrase_count',0) >= 2 else 'refine_question'\n",
    "\n",
    "def refine_question(state: AgentState):\n",
    "    if state.get('rephrase_count',0) >= 2:\n",
    "        return state\n",
    "    msgs = [SystemMessage(content='Refine INGRES query slightly'), HumanMessage(content=state['rephrased_question'])]\n",
    "    try:\n",
    "        new_q = ChatOpenAI(model='gpt-4o-mini').invoke(ChatPromptTemplate.from_messages(msgs).format()).content.strip()\n",
    "    except Exception:\n",
    "        new_q = state['rephrased_question'] + ' (please be more specific)'\n",
    "    state['rephrased_question'] = new_q\n",
    "    state['rephrase_count'] = state.get('rephrase_count',0) + 1\n",
    "    return state\n",
    "\n",
    "def generate_answer(state: AgentState):\n",
    "    context_text = '\\n\\n---\\n\\n'.join([d.page_content for d in state.get('documents',[])])\n",
    "    history = '\\n'.join([m.content for m in state.get('messages',[])])\n",
    "    try:\n",
    "        res = ChatOpenAI(model='gpt-4o-mini').invoke({'history': history, 'context': context_text, 'question': state['rephrased_question']})\n",
    "        content = res.content.strip()\n",
    "    except Exception as e:\n",
    "        logger.exception('RAG chain failed: %s', e)\n",
    "        content = 'I found the following relevant excerpts:\\n' + '\\n---\\n'.join([d.page_content[:400] for d in state.get('documents',[])])\n",
    "    state.setdefault('messages',[]).append(AIMessage(content=content))\n",
    "    return state\n",
    "\n",
    "def gis_classifier(state: AgentState):\n",
    "    last = state['messages'][-1].content if state.get('messages') else ''\n",
    "    msgs = [SystemMessage(content='Need GIS map? Yes/No'), HumanMessage(content=f\"{state['rephrased_question']}\\n\\n{last}\")]\n",
    "    try:\n",
    "        r = ChatOpenAI(model='gpt-4o-mini').with_structured_output(GISRequest).invoke(ChatPromptTemplate.from_messages(msgs).format())\n",
    "        state['needs_gis'] = r.needs_gis.strip().lower() == 'yes'\n",
    "        state['gis_data'] = {'location': r.location_data.strip()} if state['needs_gis'] else {}\n",
    "    except Exception:\n",
    "        q = (state.get('rephrased_question','') + ' ' + last).lower()\n",
    "        state['needs_gis'] = any(w in q for w in ['map','location','district','lat','lon','show me','plot'])\n",
    "        state['gis_data'] = {'location': state.get('rephrased_question','')} if state['needs_gis'] else {}\n",
    "    return state\n",
    "\n",
    "def generate_gis_map(state: AgentState):\n",
    "    if not state.get('needs_gis'):\n",
    "        return state\n",
    "    pts = [\n",
    "        {\"lat\":15.3, \"lon\":75.7, \"val\":200},\n",
    "        {\"lat\":19.0, \"lon\":77.0, \"val\":250}\n",
    "    ]\n",
    "    m = folium.Map(location=[22,79], zoom_start=5)\n",
    "    for p in pts:\n",
    "        folium.CircleMarker([p['lat'], p['lon']], radius=5 + p['val']*0.01, popup=str(p['val'])).add_to(m)\n",
    "    state['map_html'] = m._repr_html_()\n",
    "    last = state['messages'][-1]\n",
    "    state['messages'][-1] = AIMessage(content=last.content + '\\n\\n[GIS Map Attached]')\n",
    "    return state\n",
    "\n",
    "def cannot_answer(state: AgentState):\n",
    "    state.setdefault('messages',[]).append(AIMessage(content=\"I'm sorry, I can't find that.\"))\n",
    "    return state\n",
    "\n",
    "def off_topic_response(state: AgentState):\n",
    "    state.setdefault('messages',[]).append(AIMessage(content=\"I'm sorry, I cannot answer this.\"))\n",
    "    return state\n",
    "\n",
    "# ---------------------------\n",
    "# Build LangGraph workflow\n",
    "# ---------------------------\n",
    "cp = MemorySaver()\n",
    "wf = StateGraph(AgentState)\n",
    "\n",
    "nodes = [\n",
    "    'question_rewriter','question_classifier','off_topic_response',\n",
    "    'retrieve','retrieval_grader','generate_answer',\n",
    "    'refine_question','cannot_answer',\n",
    "    'gis_classifier','generate_gis_map'\n",
    "]\n",
    "for n in nodes:\n",
    "    wf.add_node(n, globals()[n])\n",
    "\n",
    "wf.set_entry_point('question_rewriter')\n",
    "wf.add_edge('question_rewriter','question_classifier')\n",
    "wf.add_conditional_edges('question_classifier', on_topic_router, {'retrieve':'retrieve','off_topic_response':'off_topic_response'})\n",
    "wf.add_edge('retrieve','retrieval_grader')\n",
    "wf.add_conditional_edges('retrieval_grader', proceed_router, {'generate_answer':'generate_answer','refine_question':'refine_question','cannot_answer':'cannot_answer'})\n",
    "wf.add_edge('refine_question','retrieve')\n",
    "wf.add_edge('generate_answer','gis_classifier')\n",
    "wf.add_conditional_edges('gis_classifier', lambda s: 'generate_gis_map' if s.get('needs_gis') else END, {'generate_gis_map':'generate_gis_map', END:END})\n",
    "wf.add_edge('generate_gis_map', END)\n",
    "wf.add_edge('cannot_answer', END)\n",
    "wf.add_edge('off_topic_response', END)\n",
    "\n",
    "graph = wf.compile(checkpointer=cp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21bfe3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To use this code, run your full script first to compile the graph.\n",
    "# Then you can run the following lines in your environment.\n",
    "\n",
    "# Create an interactive loop to chat with the graph\n",
    "# For a single session, you can use a fixed thread_id.\n",
    "# For a real application, you would generate a unique ID per user.\n",
    "thread_id = \"my-ingres-chatbot-session-1\"\n",
    "\n",
    "while True:\n",
    "    user_query = input(\"You: \")\n",
    "    if user_query.lower() in [\"exit\", \"quit\", \"q\"]:\n",
    "        print(\"Goodbye!\")\n",
    "        break\n",
    "\n",
    "    try:\n",
    "        # Pass the 'configurable' dictionary with the thread_id\n",
    "        final_state = graph.invoke(\n",
    "            {\"question\": HumanMessage(content=user_query)},\n",
    "            config={\"configurable\": {\"thread_id\": thread_id}}\n",
    "        )\n",
    "        \n",
    "        last_message = final_state['messages'][-1]\n",
    "        print(f\"Chatbot: {last_message.content}\")\n",
    "        \n",
    "        if final_state.get('map_html'):\n",
    "            print(\"\\n[GIS map generated]\\n\")\n",
    "            with open(\"gis_map.html\", \"w\") as f:\n",
    "                f.write(final_state['map_html'])\n",
    "            print(\"Map saved as gis_map.html. Open it in your browser to view.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e36a65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
